{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two files:\n",
    "model_outputs = json.load(open('/Users/venkatakesavvenna/Sem6/RSAI/Mid_Project/dataset/nli_output/chatgpt_checker_final.json'))\n",
    "human_outputs = json.load(open('/Users/venkatakesavvenna/Sem6/RSAI/Mid_Project/dataset/human_annotations/zero_context/nq_chatgpt_answers.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'response', 'triplets', 'reference', 'Y', 'ys'])\n",
      "dict_keys(['id', 'response', 'claude2_response_kg'])\n"
     ]
    }
   ],
   "source": [
    "# Print the keys \n",
    "print(model_outputs[0].keys())\n",
    "print(human_outputs[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an array of the form [triplet, model_output, human_output]\n",
    "final_array = []\n",
    "\n",
    "# Check the ids, that are common in both the files\n",
    "for model_output in model_outputs:\n",
    "    for human_output in human_outputs:\n",
    "        if model_output['id'] == human_output['id']:\n",
    "            final_array.append([model_output[\"triplets\"], model_output[\"ys\"], [entry[\"human_label\"] for entry in human_output[\"claude2_response_kg\"]]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 45.  10.  15.]\n",
      " [ 31. 144.  11.]\n",
      " [ 13.   8.  29.]]\n",
      "\n",
      "Accuracy: 0.7124183006535948\n",
      "\n",
      "Category: Neutral\n",
      "Precision: 0.5056179775280899\n",
      "Recall: 0.6428571428571429\n",
      "F1-score: 0.5660377358490566\n",
      "\n",
      "Category: Entailment\n",
      "Precision: 0.8888888888888888\n",
      "Recall: 0.7741935483870968\n",
      "F1-score: 0.8275862068965517\n",
      "\n",
      "Category: Contradiction\n",
      "Precision: 0.5272727272727272\n",
      "Recall: 0.58\n",
      "F1-score: 0.5523809523809523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Categories\n",
    "categories = ['Neutral', 'Entailment', 'Contradiction']\n",
    "\n",
    "# Initialize confusion matrix\n",
    "conf_matrix = np.zeros((len(categories), len(categories)))\n",
    "\n",
    "# Count occurrences of each combination of model and human outputs\n",
    "for entry in final_array:\n",
    "    model_outputs = entry[1]\n",
    "    human_outputs = entry[2]\n",
    "\n",
    "    for model_output, human_output in zip(model_outputs, human_outputs):\n",
    "        # Get indices of categories\n",
    "        model_index = categories.index(model_output)\n",
    "        human_index = categories.index(human_output)\n",
    "\n",
    "        # Update confusion matrix\n",
    "        conf_matrix[model_index][human_index] += 1\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "print(f\"\\nAccuracy: {accuracy}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score for each category\n",
    "for i, category in enumerate(categories):\n",
    "    true_positives = conf_matrix[i][i]\n",
    "    false_positives = sum(conf_matrix[:, i]) - true_positives\n",
    "    false_negatives = sum(conf_matrix[i, :]) - true_positives\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching: 74 out of 94\n",
      "Accuracy: 78.72340425531915%\n"
     ]
    }
   ],
   "source": [
    "# Analysis for Alpaca-7B model\n",
    "human_outputs = json.load(open('dataset/human_annotations/zero_context/nq_llama2_70b_chat_answers.json'))\n",
    "model_outputs = json.load(open('dataset/nli_output/llama-70b_checker_final.json'))\n",
    "\n",
    "\n",
    "# Make an array of the form [triplet, model_output, human_output]\n",
    "final_array = []\n",
    "\n",
    "# Take the majority votes for the human outputs\n",
    "for model_output in model_outputs:\n",
    "    for human_output in human_outputs:\n",
    "        if model_output['id'] == human_output['id']:\n",
    "            model_final = [max(set(model_output[\"ys\"]), key = model_output[\"ys\"].count)]\n",
    "            human_final = [max(set([entry[\"human_label\"] for entry in human_output[\"claude2_response_kg\"]]), key = [entry[\"human_label\"] for entry in human_output[\"claude2_response_kg\"]].count)]\n",
    "\n",
    "            final_array.append([model_output[\"triplets\"], model_final, human_final])\n",
    "\n",
    "# Check if the models, human outputs are matching\n",
    "matching = 0\n",
    "\n",
    "for entry in final_array:\n",
    "    if entry[1] == entry[2]:\n",
    "        matching += 1\n",
    "\n",
    "print(f\"Matching: {matching} out of {len(final_array)}\")\n",
    "\n",
    "accuracy = matching * 100 / len(final_array)\n",
    "print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Contradiction']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Entailment']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Contradiction']\n",
      "['Contradiction']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Entailment']\n",
      "['Contradiction']\n",
      "['Contradiction']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Contradiction']\n",
      "['Neutral']\n",
      "['Entailment']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Entailment']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Entailment']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Entailment']\n",
      "['Neutral']\n",
      "['Contradiction']\n",
      "['Entailment']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Entailment']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Entailment']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Entailment']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Entailment']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "Entailments: 11.702127659574469\n",
      "Contradictions: 7.446808510638298\n",
      "Neutrals: 80.85106382978724\n"
     ]
    }
   ],
   "source": [
    "# on the human annotated data count the number of entailmet=nts, neutral and contradiction\n",
    "entailments = 0\n",
    "contradictions = 0\n",
    "neutrals = 0\n",
    "\n",
    "for entry in final_array:\n",
    "    print(entry[2])\n",
    "    if entry[2] == [\"Entailment\"]:\n",
    "        entailments += 1\n",
    "    elif entry[2] == [\"Contradiction\"]:\n",
    "        contradictions += 1\n",
    "    else:\n",
    "        neutrals += 1\n",
    "\n",
    "print(f\"Entailments: {entailments*100/len(final_array)}\")\n",
    "print(f\"Contradictions: {contradictions*100/len(final_array)}\")\n",
    "print(f\"Neutrals: {neutrals*100/len(final_array)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
